{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Edit data directory here\n",
    "DATA_DIR = \"./input/techjam\"\n",
    "\n",
    "\n",
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.percentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_\n",
    "\n",
    "def get_prep_data(DATA_DIR):\n",
    "    # Reading Files\n",
    "    cc = pd.read_csv(os.path.join(DATA_DIR,'cc.csv'),parse_dates=['pos_dt'])\n",
    "    demo = pd.read_csv(os.path.join(DATA_DIR,'demographics.csv'))\n",
    "    kplus = pd.read_csv(os.path.join(DATA_DIR,'kplus.csv'),parse_dates=['sunday'])\n",
    "\n",
    "    train = pd.read_csv(os.path.join(DATA_DIR,'train.csv'))\n",
    "    test = pd.read_csv(os.path.join(DATA_DIR,'test.csv'))\n",
    "\n",
    "    # Set-up\n",
    "    cc_mapper = demo[['id','cc_no']].copy()\n",
    "    demo = demo.drop('cc_no',axis=1).drop_duplicates().reset_index(drop=True)\n",
    "    label = pd.concat([train,test],axis=0,ignore_index=True)\n",
    "    demo = demo.merge(label, on='id')\n",
    "    demo['ocp_cd'] = demo['ocp_cd'].fillna(0).astype(int)\n",
    "    demo.set_index('id',inplace=True)\n",
    "    kplus.set_index('id',inplace=True)\n",
    "    joined_cc = cc.merge(cc_mapper, on='cc_no', how='inner').drop('cc_no', axis=1)\n",
    "\n",
    "    kplus['month'] = kplus['sunday'].dt.month\n",
    "    kplus['month'] = 'month'+ kplus['month'].astype(str)\n",
    "    joined_cc['month'] = joined_cc.pos_dt.dt.month\n",
    "    joined_cc['month'] = 'month'+ joined_cc['month'].astype(str)\n",
    "\n",
    "    bank_holidays = ['2018-01-01','2018-01-02','2018-03-01','2018-04-06','2018-04-13',\n",
    "                    '2018-04-14','2018-04-15','2018-04-16','2018-05-01','2018-05-29']\n",
    "    joined_cc['is_holiday'] = joined_cc['pos_dt'].isin([datetime.datetime.strptime(i, '%Y-%m-%d') for i in bank_holidays]).astype(int)\n",
    "    joined_cc['is_weekend'] = joined_cc['pos_dt'].dt.weekday.isin([0,6]).astype(int)\n",
    "    joined_cc['is_holiday'] = 'holiday'+joined_cc['is_holiday'].astype(str)\n",
    "    joined_cc['is_weekend'] = 'weekend'+joined_cc['is_weekend'].astype(str)\n",
    "    joined_cc['quarter'] = 'q'+((joined_cc['pos_dt'].dt.month>=4)+1).astype(str)\n",
    "\n",
    "\n",
    "    # Adding log \n",
    "    demo['income'] = demo['income'].apply(np.log1p)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    # Target Encoding\n",
    "    demo = demo.reset_index()\n",
    "    demo['cc_cnt'] = demo['id'].map(cc_mapper.groupby('id').cc_no.count())\n",
    "    demo['has_kp'] = demo['id'].isin(kplus.index).astype(int)\n",
    "\n",
    "    # Crossing categorical features as another feature [374 / 336]\n",
    "    demo['age_gnd'] = demo['gender'].astype(str)+demo['age'].astype(str)\n",
    "    demo['gnd_ocp'] = demo['gender'].astype(str)+demo['ocp_cd'].astype(str)\n",
    "    demo['age_ocp'] = demo['age'].astype(str)+demo['ocp_cd'].astype(str)\n",
    "\n",
    "    # Left age out of categorical features since it's ordinal\n",
    "    categorical_features = ['gender','ocp_cd','age_gnd','gnd_ocp','age_ocp']\n",
    "\n",
    "    # Target Encoding, code modified from [374]\n",
    "    for feature in categorical_features + ['age']:\n",
    "        means_per_group = demo[demo['income']>0].groupby(feature)['income'].mean()\n",
    "        demo[feature+'_mean'] = demo[feature].map(means_per_group)\n",
    "\n",
    "        count_per_group = demo[demo['income']>0].groupby(feature)['income'].count()\n",
    "        demo[feature+'_count'] = demo[feature].map(count_per_group)\n",
    "    demo.set_index('id',inplace=True)\n",
    "    \n",
    "     # Preping Training data\n",
    "    train = demo.copy()\n",
    "\n",
    "    # Normal Total Groupby\n",
    "    kplus_tot = kplus.groupby('id').agg({'kp_txn_count':'sum','kp_txn_amt':'sum'}).copy()\n",
    "    kplus_mm_tot = kplus.groupby(['id','month']).agg({'kp_txn_count':'sum','kp_txn_amt':'sum'}).unstack(level=1).copy()\n",
    "    kplus_mm_tot.columns = ['_'.join([str(c) for c in lst]) for lst in kplus_mm_tot.columns]\n",
    "\n",
    "    # CreditCard Total Groupby\n",
    "    cc_tot = joined_cc.groupby('id').agg({'cc_txn_amt':['count','sum']}).copy()\n",
    "    cc_tot.columns = ['_'.join(i) for i in cc_tot.columns]\n",
    "\n",
    "    # CreditCard Monthly Groupby\n",
    "    combined_cc = pd.pivot_table(joined_cc, index= 'id', columns= 'month', values= 'cc_txn_amt', aggfunc= [np.mean, min, max, np.sum, 'count', np.var, percentile(10), percentile(90)])\n",
    "    combined_cc.columns = ['cc_'+'_'.join([str(c) for c in lst]) for lst in combined_cc.columns]\n",
    "\n",
    "\n",
    "    # CreditCard Pompus Features\n",
    "    combined_cc_holiday = pd.pivot_table(joined_cc, index= 'id', columns= 'is_holiday', values= 'cc_txn_amt', aggfunc= [np.mean, min, max, np.sum, 'count', np.var, percentile(10), percentile(90)])\n",
    "    combined_cc_weekend = pd.pivot_table(joined_cc, index= 'id', columns= 'is_weekend', values= 'cc_txn_amt', aggfunc= [np.mean, min, max, np.sum, 'count', np.var, percentile(10), percentile(90)])\n",
    "    combined_cc_quarter = pd.pivot_table(joined_cc, index= 'id', columns= 'quarter', values= 'cc_txn_amt', aggfunc= [np.mean, min, max, np.sum, 'count', np.var, percentile(10), percentile(90)])\n",
    "    combined_cc_holiday.columns = ['cc_'+'_'.join([str(c) for c in lst]) for lst in combined_cc_holiday.columns]\n",
    "    combined_cc_weekend.columns = ['cc_'+'_'.join([str(c) for c in lst]) for lst in combined_cc_weekend.columns]\n",
    "    combined_cc_quarter.columns = ['cc_'+'_'.join([str(c) for c in lst]) for lst in combined_cc_quarter.columns]\n",
    "\n",
    "    # Joining all together\n",
    "    train = train.join(kplus_tot).join(kplus_mm_tot).join(cc_tot).join(combined_cc).join(combined_cc_holiday).join(combined_cc_weekend).join(combined_cc_quarter).fillna(0)\n",
    "\n",
    "    # Quick Normalization\n",
    "#     for col in [ i for i in train.columns if i not in ['id','gender','ocp_cd','age','income']]:\n",
    "#         train[col] = StandardScaler().fit_transform(train[col].values.reshape(-1, 1))\n",
    "#         train[col] = train[col].fillna(0)\n",
    "#         if 'amt' in col:\n",
    "#             train[col] = train[col].apply(np.log1p)\n",
    "\n",
    "    X_train = train[train['income']>0].drop('income',axis=1).copy()\n",
    "    y_train = pd.DataFrame(train[train['income']>0]['income']).copy()\n",
    "    X_test = train[train['income']<=0].drop('income',axis=1).copy()\n",
    "\n",
    "    return X_train, y_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test =  get_prep_data(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feature = ['gender','ocp_cd','age_gnd','gnd_ocp','age_ocp', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def techjam_score(y_pred, y_true):\n",
    "    y_pred = np.array(y_pred)\n",
    "    y_true = np.array(y_true)\n",
    "    \n",
    "    return 100 - 100 * np.mean((y_pred-y_true) ** 2 / (np.minimum(2*y_true, y_pred) + y_true)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def techjam_feval_log(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'techjam_score', techjam_score(np.exp(y_pred), np.exp(y_true)), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in cat_feature:\n",
    "    X_test[cat] =X_test[cat].astype(int)\n",
    "    X_train[cat] =X_train[cat].astype(int)\n",
    "\n",
    "train_data = lightgbm.Dataset(X_train, label=y_train, categorical_feature=cat_feature , free_raw_data=False)\n",
    "\n",
    "num_leaves_choices = [15, 31, 63, 127, 200, 255, 300, 350, 400,511 ,600]\n",
    "ft_frac_choices = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "bagging_frac_choices = [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# We will store the cross validation results in a simple list,\n",
    "# with tuples in the form of (hyperparam dict, cv score):\n",
    "cv_results = []\n",
    "\n",
    "\n",
    "for num_lv in tqdm_notebook(num_leaves_choices):\n",
    "    for bg_fac in bagging_frac_choices:\n",
    "        for ft_fac in ft_frac_choices:\n",
    "            hyperparams = {\"boosting_type\":'gbdt',\n",
    "                            \"objective\": 'mape',\n",
    "                            \"metrics\": 'None',\n",
    "                            \"num_leaves\": num_lv,\n",
    "                            \"feature_fraction\": ft_fac,\n",
    "                            \"bagging_fraction\": bg_fac,\n",
    "                            \"learning_rate\": 0.01\n",
    "                                     }\n",
    "            validation_summary = lightgbm.cv(hyperparams,\n",
    "                                            train_data,\n",
    "                                            num_boost_round=10000,\n",
    "                                            nfold=5,\n",
    "                                            feval=techjam_feval_log,\n",
    "                                            stratified=False,\n",
    "                                            shuffle=True,\n",
    "                                            early_stopping_rounds=50,\n",
    "                                            verbose_eval=10)\n",
    "            \n",
    "            optimal_num_trees = len(validation_summary[\"techjam_score-mean\"])\n",
    "            \n",
    "            # to the hyperparameter dictionary:\n",
    "            hyperparams[\"num_boost_round\"] = optimal_num_trees\n",
    "\n",
    "           # And we append results to cv_results:\n",
    "            cv_results.append((hyperparams, validation_summary[\"techjam_score-mean\"][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cv_result = sorted(cv_results, key=lambda tup:tup[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_cv_result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select parameter score > 92.21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### select best 10 models\n",
    "MODELS=[] \n",
    "for params_and_score in tqdm_notebook(sort_cv_result[-10:]):\n",
    "    params = params_and_score[0]\n",
    "    model = lightgbm.train(params,\n",
    "                train_data,\n",
    "               )\n",
    "    MODELS.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ensemble 10 models \n",
    "pred = []\n",
    "for model in MODELS:\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.exp(y_pred)\n",
    "    pred.append(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=np.array(pred)\n",
    "# perform ensemble\n",
    "final_pred = pred.mean(axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create submission dataframe\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = [i for i in range(50001,65001)] \n",
    "submission['final_pred'] = final_pred"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
